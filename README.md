# ollama_chainlit
Run locally Ollama models in a nice chainlit user interface.

Requirements : docker/docker-compose
