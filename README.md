# ollama_chainlit
Run locally Ollama models in a nice chainlit user interface.
